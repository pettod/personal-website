<!doctype html>
<html lang="en" data-bs-theme="auto" style="height: 100%;">
  <head>
    <script src="assets/js/color-modes.js"></script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Backpropagation, deepfake detection, segmentation, motion prediction, autonomous vehicles, molecular translation">
    <title>Projects</title>
    <link rel="icon" href="assets/pt_icon_dark_small.png" type="image/x-icon">
    <link href="assets/dist/css/bootstrap.min.css" rel="stylesheet">

    <style>
      .img-border {
        border-radius: 20px;
        margin-bottom: 15px;
        cursor: pointer; /* Change cursor to pointer for images/videos */
      }

      .aspect-ratio-keeper {
        position: relative;
        width: 100%;
        padding-bottom: 56.25%; /* 16:9 aspect ratio */
        overflow: hidden;
      }

      .aspect-ratio-keeper iframe {
        position: absolute;
        width: 100%;
        height: 100%;
        border: 0;
      }

      a {
        text-decoration: none;
      }

      .modal {
        display: none; /* Hidden by default */
        position: fixed; /* Stay in place */
        z-index: 1000; /* Sit on top */
        left: 0;
        top: 0;
        width: 100%; /* Full width */
        height: 100%; /* Full height */
        overflow: auto; /* Enable scroll if needed */
        background-color: rgba(0, 0, 0, 0); /* Start transparent */
        -webkit-backdrop-filter: blur(0px); /* Safari support */
        backdrop-filter: blur(0px); /* Standard blur effect */
        opacity: 0; /* Start invisible */
        transition: opacity 0.3s ease-in-out, background-color 0.3s ease-in-out, backdrop-filter 0.3s ease-in-out, -webkit-backdrop-filter 0.3s ease-in-out;
      }

      .modal.show {
        background-color: rgba(0, 0, 0, 0.8); /* End with dark background */
        -webkit-backdrop-filter: blur(4px); /* Safari support */
        backdrop-filter: blur(4px); /* Standard blur effect */
        opacity: 1; /* Fully visible */
      }

      .modal-content {
        display: block;
        max-height: 80%;
        max-width: 80%;
        object-fit: contain;
        width: auto; /* Maintain aspect ratio */
        height: auto; /* Maintain aspect ratio */
        margin: auto; /* Remove auto margin since we're using flexbox */
        transform: scale(0.8); /* Start smaller */
        opacity: 0; /* Start invisible */
        transition: transform 0.3s ease-out, opacity 0.3s ease-out;
      }

      .modal-content.show {
        transform: scale(1); /* Scale to full size */
        opacity: 1; /* Fully visible */
      }

      .close {
        position: absolute;
        top: 15px;
        right: 35px;
        color: white;
        font-size: 40px;
        font-weight: bold;
        transition: 0.3s;
      }

      .close:hover,
      .close:focus {
        color: #bbb;
        text-decoration: none;
        cursor: pointer;
      }

      .bd-text {
        font-size: 18px;
        line-height: 27px;
        font-weight: 350;
        margin-bottom: 24px;
        letter-spacing: 0px;
        font-family: sans-serif;
        text-align: left;
      }

      .img-text {
        font-size: 15px;
        line-height: 21px;
        font-weight: 250;
        margin-bottom: 24px;
        letter-spacing: 0px;
        font-family: sans-serif;
        text-align: left;
      }

      .ref-text {
        font-size: small;
        line-height: 21px;
        font-weight: 350;
        margin-bottom: 24px;
        letter-spacing: 0px;
        font-family: sans-serif;
        text-align: left;
      }

      @media (max-width: 768px) {
        .bd-text {
          font-size: 16px;
          line-height: 24px;
          margin-bottom: 16px;
        }

        .img-text {
          font-size: 13px;
          line-height: 18px;
          margin-bottom: 11px;
        }
      }
    </style>
    
    <!-- Custom styles for this template -->
    <link href="assets/dist/css/custom.css" rel="stylesheet">
  </head>
  <body style="height: 100%;">

<!-- NAVBAR -->
<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
  <div class="container">
    <span class="nav-item"><a class="nav-link" href="/"><img width="28" height="28" src="assets/pt_icon_light.png" onmouseover="this.src='assets/pt_icon_lighter.png';" onmouseout="this.src='assets/pt_icon_light.png';"></a></span>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarsExample08" aria-controls="navbarsExample08" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    
    <div class="collapse navbar-collapse justify-content-md-center" id="navbarsExample08">
      <ul class="navbar-nav">
        <li class="nav-item" style="padding-right: 125px;"><a class="nav-link" href="/">Intro</a></li>
        <li class="nav-item" style="padding-right: 125px;"><a class="nav-link" href="experience">Experience</a></li>
        <li class="nav-item" style="padding-right: 125px;"><a class="nav-link" href="projects">Projects</a></li>
        <li class="nav-item"><a class="nav-link" href="education">Education</a></li>
      </ul>
    </div>
  </div>
</nav>

<main style="max-width: 1800px; margin-left: auto; margin-right: auto;">

  <!-- Top headline -->
  <div id="topdiv" class="text-bg-dark m-md-3 text-center" style="overflow: hidden;">
    <canvas id="canvas" height="350"></canvas>
  </div>

  <!-- The first 3 projects -->
  <div class="d-md-flex flex-md-equal w-100 my-md-3 ps-md-3">
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Backpropagation</h2>
        <img class="img-border" width="100%" src="https://user-images.githubusercontent.com/33998401/274561674-b0de8b2e-3e7a-4538-befc-aec0873632de.gif" onclick="openModal(this.src, 'image')">
        <p class="img-text">
          Learning process of a single neuron. Included components: 2 weights, bias, ReLU, mean squared error, and gradient descent.
        </p>
        <div class="collapse" id="collapse-backpropagation">
          <h4>Introduction</h4>
          <p class="bd-text">
            A machine learning model includes model trainable parameters ‒ weights and biases ‒ activation functions, and skip connections. The feed forward process of applying a machine learning model to inputs is called <em>Forward propagation</em>. The forward propagation in a single neuron is expressed as
          </p>
          <p class="bd-text" style="text-align: center;">
            \[o = a(w \cdot i + b)\]
          </p>
          <p class="bd-text">
            where \(o\) is an output, \(a\) is an activation function, \(w\) is a weight, \(i\) is an input, and \(b\) is a bias.
          </p>
          <p class="bd-text">
            Training a machine learning model requires a learning phase to update the weights and biases. This learning phase is called <em><a href="https://en.wikipedia.org/wiki/Backpropagation">Backpropagation</a></em>.
          </p>
          <h4>Implementation</h4>
          <p class="bd-text" style="margin-bottom: 0px;">
            In this project, I got into <em><a href="https://en.wikipedia.org/wiki/First_principle">First principles</a></em> of deep learning. I coded an own implementation of a neural network framework without using any deep learning frameworks. The features which I implemented in Python were:
          </p>
          <ul class="bd-text" style="text-align: left;">
            <li>General</li>
            <ul class="bd-text" style="text-align: left; margin-bottom: 0px;">
              <li>Forward propagation</li>
              <li>Backpropagation</li>
              <li>Network visualization</li>
            </ul>
            <li>Layers</li>
            <ul class="bd-text" style="text-align: left; margin-bottom: 0px;">
              <li>Fully connected</li>
            </ul>
            <li>Activation functions</li>
            <ul class="bd-text" style="text-align: left; margin-bottom: 0px;">
              <li>ReLU</li>
              <li>Softmax</li>
            </ul>
            <li>Loss functions</li>
            <ul class="bd-text" style="text-align: left; margin-bottom: 0px;">
              <li>Mean squared error</li>
              <li>Cross entropy loss</li>
            </ul>
            <li>Optimizers</li>
            <ul class="bd-text" style="text-align: left; margin-bottom: 0px;">
              <li>Gradient descent</li>
            </ul>
          </ul>
          <img class="img-border" width="100%" src="https://github.com/pettod/backpropagation/assets/33998401/622219bd-f0d4-4485-acf7-1f11157d614e" onclick="openModal(this.src, 'image')">
          <p class="img-text">
            The code is scalable. It trains and draws any size of a network.
          </p>  
          <h4>Theory</h4>
          <p class="bd-text">
            The main principle behind backpropagation is <em><a href="https://en.wikipedia.org/wiki/Derivative">Derivative</a></em>. To define how to update a neural network during an iteration, one needs to compute a gradient for each parameter. The gradient determines how much a parameter affects the loss, e.g. a parameter with a gradient 0 contributes nothing to the loss. The gradient of a parameter \(g_{p}\) is defined by the derivative of the loss \(l\) with respect to the parameter \(p\):
          </p>
          <p class="bd-text" style="text-align: center;">
            \[g_{p} = \frac{dl}{dp}\]
          </p>
          <p class="bd-text">
            To compute the gradient of a parameter which has operations between the loss and the parameter, require <em><a href="https://en.wikipedia.org/wiki/Chain_rule">Chain rule</a></em>. The chain rule expresses the derivative of the composition of two differentiable functions in terms of the derivatives of the two separate functions. By knowing the derivative of a loss \(l\) with respect to a weight \(w_{1}\) and the derivate of the weight \(w_{1}\) with respect to a weight \(w_{0}\), then the derivative of \(l\) with respect to \(w_{0}\) will be:
          </p>
          <p class="bd-text" style="text-align: center;">
            \[\frac{dl}{dw_{0}} = \frac{dw_{1}}{dw_{0}} \cdot \frac{dl}{dw_{1}}\]
          </p>
          <p class="bd-text">
            To find the derivative of a weight multiplication and a bias addition functions, we could use the definition of a differentiable function and calculate <em><a href="https://en.wikipedia.org/wiki/Limit_(mathematics)">Limit</a></em> \(L\):
          </p>
          <p class="bd-text" style="text-align: center;">
            \[L = \lim\limits_{h \to 0} \frac{f(a + h) - f(a)}{h}\]
          </p>
          <p class="bd-text">
            In a weight multiplication, the input \(i\) is multiplied by a weight \(w\), hence the function will be \(f(w) = iw\). Now, we can use the definition of a differentiable function and calculate the derivative \(f'(w)\):
          </p>
          <p class="bd-text" style="text-align: center;">
            $$
            \begin{align}
              f'(w) &= \lim\limits_{h \to 0} \frac{i(w+h) - iw}{h}\\
              &= \lim\limits_{h \to 0} \frac{iw + ih - iw}{h}\\
              &= \lim\limits_{h \to 0} \frac{ih}{h}\\
              &= i
            \end{align}
            $$
          </p>
          <p class="bd-text">
            As seen, the "local derivative" of an input multiplied by a weight with respect to the weight is the value of the input. To obtain the final derivative which we need ‒ the derivative of the loss with respect to the weight ‒ the "local derivative" has to be multiplied with the rest of the chain rule derivatives.
          </p>
          <p class="bd-text">
            Similarly, let's derive the local derivative of a bias addition. In a neuron, a bias \(b\) is added to the outputs of weight multiplications \(o\). Thus, the derivate of a function \(f(b) = o + b\) will be: 
          </p>
          <p class="bd-text" style="text-align: center;">
            $$
            \begin{align}
              f'(b) &= \lim\limits_{h \to 0} \frac{o + (b+h) - (o+b)}{h}\\
              &= \lim\limits_{h \to 0} \frac{o + b + h - o - b}{h}\\
              &= \lim\limits_{h \to 0} \frac{h}{h}\\
              &= 1
            \end{align}
            $$
          </p>
          <p class="bd-text">
            Here, the local derivative is 1. Therefore, the derivative of the loss with respect to a bias is 1 multiplied by the rest of the chain rule derivatives.
          </p>
          <p class="bd-text">
            The derivatives of complex activation functions one could just search online and implement into the code. That's what I did.
          </p>
          <p class="bd-text">
            Finally, when the gradients are computed for each parameter, it's time to apply them and update the parameters. The simplest optimizer used to apply the gradients is <em><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient descent</a></em>. The derivative shows the direction of growth but when training a neural network, we want to minimize the loss hence take a step (<em><a href="https://en.wikipedia.org/wiki/Learning_rate">Learning rate</a></em>) to the direction of a negative gradient. That means, using gradient descent the updated value of a parameter \(p\) will be:
            \[p_{u} = - g_{p} \cdot r \cdot p_{o}\]
            where \(p_{u}\) is the updated value of a parameter, \(g_{p}\) is the gradient of the parameter, \(r\) is the learning rate, and \(p_{o}\) is the old value of the parameter.
          </p>
          <p class="bd-text">
            The last piece of an iteration is to zero gradients. As the gradients have already been applied to the parameters, we do not want to keep them anymore accumulating in the training process.
          </p>
          <p class="bd-text">
            There you have it, a comprehensive description of how neural networks are built and how they learn. This text explained all the necessary steps required to train a neural network successfully. To enhance the results, there are multiple different components offered which all rely on the theory of derivate.
          </p>
          <h4>Interesting findings</h4>
          <p class="bd-text">
            While debugging and visualizing the reason why my network didn't learn I came across <em><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Potential_problems">Dying ReLU</a></em> problem. Some units may always output zero (they are "dead" neurons) and don't update their weights during training. To address this problem, variations of ReLU have been proposed, such as Leaky ReLU, Parametric ReLU (PReLU), and Exponential Linear Unit (ELU), which aim to allow a small gradient for negative inputs to prevent neurons from becoming completely inactive.
          </p>
          <p class="ref-text">
            2023 | <a href="https://github.com/pettod/backpropagation">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-backpropagation" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-backpropagation" aria-expanded="false" aria-controls="collapse-backpropagation">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">PDF sensitive data redaction MCP</h2>
        <video class="img-border" width="100%" autoplay loop muted src="https://github-videos.s3.eu-west-2.amazonaws.com/masquerade.mp4" onclick="openModal(this.src, 'video')"></video>
        <p class="img-text">
          Redact sensitive data from PDF files before sending them to an LLM.
        </p>
        <div class="collapse" id="collapse-masquerade">
          <p class="bd-text">
            <h4>Problem</h4>
            <p class="bd-text">
              Tools like Claude or GPT are incredibly powerful, but they require raw input.
              If you're dealing with contracts, medical records, or internal documents, that's risky.
            </p>
            <h4>Solution</h4>
            <p class="bd-text">
              Masquerade acts as a privacy firewall for your files.
              Just paste in the file path to a PDF, and Masquerade will:
            </p>
            <ul class="bd-text" style="text-align: left;">
              <li>Automatically detect sensitive data (names, emails, dates, entities)</li>
              <li>Redact the sensitive data</li>
              <li>Let you preview before sending to an LLM</li>
            </ul>
          </p>
          <img class="img-border" width="100%" src="https://github.com/user-attachments/assets/96002c8b-5839-4499-814e-e603d95e7c82" onclick="openModal(this.src, 'image')">
          <p class="img-text">
            Architecture of the MCP.
            Powered by Tinfoil API (YC X25) for secure processing.
          </p>
          <p class="ref-text">
            2025 | <a href="https://github.com/pettod/masquerade">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-masquerade" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-masquerade" aria-expanded="false" aria-controls="collapse-masquerade">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">AI companion</h2>
        <video class="img-border" width="100%" autoplay loop muted src="https://github.com/user-attachments/assets/807f06ba-5ba8-43bf-b995-f392e4a88d4c" onclick="openModal(this.src, 'video')"></video>
        <p class="img-text">
          Talk with a voice clone of your loved one.
          The AI is personalised to have knowledge of your background.
        </p>
        <div class="collapse" id="collapse-otherly">
          <p class="bd-text">
            Elderly people are often lonely and in a risk of getting memory diseases such as Alzheimer's and dementia.
            Giving them an option to talk with AI can reduce the cognitive decline.
            Having voice clone of their loved ones was a requested feature especially when they didn't feel safe at home.
          </p>
          <img class="img-border" width="100%" src="assets/voice_architechture.png" onclick="openModal(this.src, 'image')">
          <p class="img-text">
            Architecture of the voice clone.
            Phone calling and web app were created for easy access.
          </p>
          <p class="ref-text">
            2025 | <a href="https://github.com/pettod/ai-companion">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-otherly" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-otherly" aria-expanded="false" aria-controls="collapse-otherly">Read more &darr;</button></p>
      </div>
    </div>
  </div>

  <!-- The second 3 projects -->
  <div class="d-md-flex flex-md-equal w-100 my-md-3 ps-md-3">
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Microscopy image analysis</h2>
        <video class="img-border" width="100%" autoplay loop muted src="https://github.com/user-attachments/assets/19562dab-f550-4f52-bb0f-8f4f2009ca8e" onclick="openModal(this.src, 'video')"></video>
        <p class="img-text">
          Analyse microscopy images using natural language.
          Segment cells and nuclei and measure their area.
        </p>
        <div class="collapse" id="collapse-microscopy">
          <p class="bd-text">
            Image analysis in drug discovery is typically done manually using ImageJ.
            Machine Learning tools are sometimes used.
            However, research scientists are not trained engineers to install or re-train ML models.
            In this project, the aim was to give research scientists access to ML model via natural language.
            However, we found out that people wouldn't pay for such software, they want a full package which is in the vision below.
            Also, they spent far less hours a year on image analysis than what was our hypothesis.
          </p>
          <img class="img-border" width="100%" src="assets/research_loop.png" onclick="openModal(this.src, 'image')">
          <p class="img-text">Research loop of drug discovery.</p>
          <p class="bd-text">
            The vision was to make an "AI Scientist" to automate drug discovery including all the steps mentioned in the image above.
          </p>
          <p class="ref-text">
            2025 | <a href="https://todorov.fi/products/microscopy">Project page</a>
            <br>
            2025 | <a href="https://github.com/pettod/medical-image-enhancement/">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-ai-scientist" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-microscopy" aria-expanded="false" aria-controls="collapse-microscopy">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Acoustic occupancy sensor</h2>
        <img class="img-border" width="100%" src="assets/rauha_flow_chart.png" onclick="openModal(this.src, 'image')">
        <p class="img-text">
          Architecture of our ultrasonic occupancy sensor.
          The sensor is based on a microphone and filters out speech frequencies to preserve privacy.
        </p>
        <div class="collapse" id="collapse-rauha">
          <p class="bd-text">
            Occupancy sensor market has couple problems:
            <ul class="bd-text" style="text-align: left;">
              <li>Cheap sensors (PIR, CO2, Desk counter) are not accurate</li>
              <li>Expensive sensors (Thermal, Depth, Lidar) are accurate but have field-of-view limitation and require installation and wirings</li>
              <li>RGB, Wi-FI, Bluetooth are not privacy preserving not accurate</li>
              <li>Battery life is a problem in many cheaper options</li>
            </ul>
          </p>
          <p class="bd-text">
            Our solution was to make plug-and-play ultrasonic sensor which is privacy preserving, have no battery life nor field-of-view problems, and is accurate.
            Our sensor based on acoustic is privacy preserved because it filters out speech frequencies.
            The device is easy to install because it can be plugged into any socket in the room as it has 360 degree field-of-view.
            Finally, it is maintenance free because it is constantly powered by the wall socket.
            <br>
            <br>
            The challenge in this project was to make the algorithm accurate because the people count, for example in a library, can be the same as in a kitchen while the acoustic signals are very different.
          </p>
          <p class="ref-text">
            2024 | <a href="https://github.com/pettod/occupancy-estimation">Github 1</a><br>
            2024 | <a href="https://github.com/pettod/microphone-occupancy">Github 2</a><br>
            2024 | <a href="https://github.com/pettod/rauha-website">Github 3</a><br>
            2024 | <a href="https://todorov.fi/products/rauha/dashboard">Dashboard</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-rauha" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-rauha" aria-expanded="false" aria-controls="collapse-deepfake">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Tone mapping</h2>
        <img class="img-border" width="100%" src="assets/tgtm.jpg" onclick="openModal(this.src, 'image')">
        <p class="img-text">
          Publication of efficient tone curve parameters estimation on embedded pixel processing pipelines.
        </p>
        <div class="collapse" id="collapse-tgtm">
          <p class="bd-text">
            Machine learning-based HDR sensor image tone mapping designed to run efficiently on ISPs.
            The work includes 8-bit SDR to 26-bit HDR dataset simulation, image data compression into image histograms, and a fully automated tone curve function parameters prediction.
            The approach is generic and could be applied on solutions with a predefined analytic function.
          </p>
          <p class="bd-text" style="margin-bottom: 0px;">
            The work in numbers:
          </p>
          <ul class="bd-text" style="text-align: left;">
            <li>26-bit HDR sensor</li>
            <li>150 dB dynamic range</li>
            <li>1k neural network parameters</li>
            <li>9 kFLOPS curve estimation</li>
            <li>53 &#956;s runtime on Raspberry Pi 4</li>
            <li>99.98 % fewer FLOPS than in <a href="https://www.mdpi.com/1424-8220/23/12/5767">DI-TM</a></li>
            <li>5.85 dB higher PSNR than in DI-TM on real HDR camera images</li>
          </ul>
          <p class="ref-text">
            2024 | <a href="https://arxiv.org/abs/2405.05016">arXiv</a><br>
            2024 | <a href="https://patents.google.com/patent/WO2025149174A1">Google patents</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-tgtm" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-tgtm" aria-expanded="false" aria-controls="collapse-tgtm">Read more &darr;</button></p>
      </div>
    </div>
  </div>

  <!-- The third 3 projects -->
  <div class="d-md-flex flex-md-equal w-100 my-md-3 ps-md-3">
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Deepfake detection</h2>
        <img class="img-border" width="100%" src="https://github.com/pettod/deepfake-recognition-kaggle/assets/33998401/f15c348e-7b6c-40af-8f16-48699016f3cd" onclick="openModal(this.src, 'image')">
        <p class="img-text">
          Real-time deepfake face detection and classification. See the moving ghost between the faces causing fake classification.*
        </p>
        <div class="collapse" id="collapse-deepfake">
          <p class="bd-text">
            Deepfake techniques, which present realistic AI-generated videos of people doing and saying fictional things, have the potential to have a significant impact on how people determine the legitimacy of information presented online.
            These content generation and modification technologies may affect the quality of public discourse and the safeguarding of human rights ‒ especially given that deepfakes may be used maliciously as a source of misinformation, manipulation, harassment, and persuasion.
          </p>
          <p class="bd-text" style="margin-bottom: 0px;">
            I created a deep learning model to identify videos with facial or voice manipulations.
            The pipeline from input to output was as follows:
          </p>
          <ol class="bd-text" style="text-align: left;">
            <li>Decode .mp4 video into frames</li>
            <li>Detect faces from multiple frames</li>
            <li>Cluster different humans in a video</li>
            <li>Remove outliers that are not real humans</li>
            <li>Classify the video by using multiple faces from different video frames</li>
          </ol>
          <p class="bd-text">
            The challenge lied in how multi-step the classification had to be.
            Before being able to classify the video, a face detection algorithm needed to be created.
            There was a time limit so the face detection had to be fast but also accurate and not every model was able to meet these requirements.
            Also, in some of the videos there were multiple people present.
            Obviously, you cannot mess the different faces because that would easily lead to a fake prediction.
            So clustering was needed.
            Then sometimes the face detection picked face looking areas such as a pillow with eyes and a mouth or a face picture on a wall.
            These also had to be filtered out to avoid misleading the classification model.
            Finally, it was time for the classification model research in which, in the end, I concatenated 9 faces to an input image and fed it to the model.
          </p>
          <p class="ref-text">
            *Video detection from the public dataset: <a href="https://arxiv.org/abs/2006.07397">https://arxiv.org/abs/2006.07397</a>
            <br>
            2020 | <a href="https://github.com/pettod/deepfake-recognition-kaggle">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-deepfake" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-deepfake" aria-expanded="false" aria-controls="collapse-deepfake">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Package delivery optimization</h2>
        <img class="img-border" width="100%" src="https://github.com/pettod/delivery-routing/assets/33998401/8bda2271-aa07-48fe-8a4d-fd0f64ebcc97" onclick="openModal(this.src, 'image')">
        <p class="img-text">
          Finding optimal route with 301 locations and 4 delivery vehicles.
        </p>
        <div class="collapse" id="collapse-package">
          <p class="bd-text" style="margin-bottom: 0px;">
            Challenged myself on learning how complex vehicle routing really is.
            The underlying problem is to minimize the traveling distance by taking into account constraints and changing environment.
            The simplest form of minimizing the distance without any constraints is called <a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem">Traveling Salesperson Problem</a> (TSP).
            Here are some numbers to understand the complexity of TSP:
          </p>
          <ul class="bd-text" style="text-align: left;" style="margin-bottom: 0px;">
            <li>With 4 locations there are (4 &ndash; 1)! = 6 possible routes</li>
            <li>With 11 locations there are (11 &ndash; 1)! = 3,628,800 possible routes</li>
            <li>With 21 locations there are (21 &ndash; 1)! = 2,432,902,008,176,640,000 possible routes</li>
          </ul>
          <p class="bd-text" style="margin-bottom: 0px;">
            Adding constraints to the routing problem further complicates the optimization.
            Constraints of the problem include, for example, vehicle capacity, vehicle refill points, traffic jams, cancelled deliveries, or product type limitations.
            I coded a solution taking into account the following constraints:
          </p>
          <ul class="bd-text" style="text-align: left;">
            <li>Minimize traveling distance</li>
            <li>N number of vehicles</li>
            <li>M number of packages</li>
            <li>Delivery deadlines of the packages</li>
            <li>Driver's working hours</li>
            <li>Driver's maximum single delivery distance</li>
            <li>Driver's mandatory lunch break</li>
          </ul>
          <p class="ref-text">
            2023 | <a href="https://github.com/pettod/delivery-routing">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-package" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-package" aria-expanded="false" aria-controls="collapse-package">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Prostate cancer grade assessment</h2>
        <img class="img-border" width="100%" src="https://github.com/pettod/personal-website/assets/33998401/ac116f86-61a6-4a3a-934e-5fd22da48c1a" onclick="openModal(this.src, 'image')">
        <p class="img-text">
          Input of 4x4 concatenated blocks of tissue area with respective predicted and ground truth Gleason segmentation.*
        </p>
        <div class="collapse" id="collapse-cancer">
          <p class="bd-text">
            With more than 1 million new diagnoses reported every year, prostate cancer (PCa) is the second most common cancer among males worldwide that results in more than 350,000 deaths annually.
            The key to decreasing mortality is developing more precise diagnostics.
          </p>
          <p class="bd-text">
            I created a deep learning model for diagnosing PCa from high-resolution images (average of 800 Mpx per image, maximum size >4000 Mpx).
          </p>
          <p class="bd-text">
            Achieved validation kappa score 0.91 (6 ISUP grades in the Gleason grading system).
            Segmented cell with normal, healthy, stroma and Gleason scores from 3 to 5.
            Classified by using as large tissue area as possible.
          </p>
          <p class="ref-text">
            *Image segmentation from the public dataset: <a href="https://www.nature.com/articles/s41591-021-01620-2">https://www.nature.com/articles/s41591-021-01620-2</a>
            <br>
            2020 | <a href="https://github.com/pettod/medical-segmentation">Github 1</a>, <a href="https://github.com/pettod/prostate-cancer-grade-assessment">Github 2</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-cancer" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-cancer" aria-expanded="false" aria-controls="collapse-cancer">Read more &darr;</button></p>
      </div>
    </div>
  </div>

  <!-- The fourth 3 projects -->
  <div class="d-md-flex flex-md-equal w-100 my-md-3 ps-md-3">
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Motion prediction for autonomous vehicles</h2>
        <img class="img-border" width="100%" src="https://github.com/pettod/personal-website/assets/33998401/d2cbc66a-544d-48ff-9a47-cd885208982e" onclick="openModal(this.src, 'image')">
        <p class="img-text">
          2D view of roads, traffic agents, and their trajectories.*
        </p>
        <div class="collapse" id="collapse-lyft">
          <p class="bd-text">
            Deep learning model for an autonomous vehicle to predict the movement of traffic agents such as cars, cyclists, and pedestrians.
            The target was to predict future trajectory from a given vector of (x,y) coordinates of the past trajectory.
          </p>
          <p class="bd-text">
            By converting the coordinates to a bird eye view image, the approach of dealing with vector data was changed to image to image problem resulting in higher accuracy.
          </p>
          <p class="ref-text">
            *Image simulated from the public dataset: <a href="https://arxiv.org/abs/2006.14480">https://arxiv.org/abs/2006.14480</a>
            <br>
            2020 | <a href="https://github.com/pettod/lyft-kaggle">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-lyft" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-lyft" aria-expanded="false" aria-controls="collapse-lyft">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Molecular translation</h2>
        <img class="img-border" width="100%" src="https://github.com/pettod/bms-molecular-translation/assets/33998401/7d1d3d97-3660-4169-ae1d-75bbb0c1a828" onclick="openModal(this.src, 'image')">
        <p class="img-text">
          Drawn Skeletal formula of an input molecule and a corresponding ground truth InChI text.*
        </p>
        <div class="collapse" id="collapse-inchi">
          <p class="bd-text">
            Sometimes the best and easiest tools are still pen and paper.
            Organic chemists frequently draw out molecular work with the <a href="https://en.wikipedia.org/wiki/Skeletal_formula">Skeletal formula</a>, a structural notation used for centuries.
            Recent publications are also annotated with machine-readable chemical descriptions (<a href="https://en.wikipedia.org/wiki/International_Chemical_Identifier">InChI</a>), but there are decades of scanned documents that can't be automatically searched for specific chemical depictions.
            To speed up research and development efforts, an automated recognition of optical chemical structures will be helpful.
          </p>
          <p class="bd-text">
            I created a deep learning model to translate chemical images to InChI text.
          </p>
          <p class="ref-text">
            *Image sample from the public dataset: <a href="https://www.kaggle.com/competitions/bms-molecular-translation/data">https://www.kaggle.com/competitions/bms-molecular-translation/data</a>
            <br>
            2021 | <a href="https://github.com/pettod/bms-molecular-translation">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-inchi" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-inchi" aria-expanded="false" aria-controls="collapse-inchi">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Charuco corner detection</h2>
        <img class="img-border" width="100%" src="https://github.com/pettod/charuco-corner-detection/raw/master/document_images/detected_charuco_markers.gif" onclick="openModal(this.src, 'image')">
        <p class="img-text">
          Real-time Charuco corner detection. All of the captured pictures (even without seeing or detecting every corner) can be used for camera calibration because every corner has an ID.
        </p>
        <div class="collapse" id="collapse-charuco">
          <p class="bd-text">
            Tool for camera calibration to detect checkerboard corners with corner identifications.
            It is especially helpful for multi-camera calibration where all the cameras cannot see the whole checkerboard.
            By knowing the corner identifications, one can use images with partly visible checkerboards for camera calibration.
          </p>
          <p class="bd-text">
            Used Python OpenCV and embedded it into Matlab as well.
          </p>
          <p class="ref-text">
            2019 | <a href="https://github.com/pettod/charuco-corner-detection">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-charuco" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-charuco" aria-expanded="false" aria-controls="collapse-charuco">Read more &darr;</button></p>
      </div>
    </div>
  </div>

  <!-- The fifth 3 projects -->
  <div class="d-md-flex flex-md-equal w-100 my-md-3 ps-md-3">
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6 text-center">GPU usage visualization</h2>
        <div class="img-border aspect-ratio-keeper">
          <iframe src="https://www.youtube.com/embed/qLBvez84VoA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="img-text">
          Real-time GPU usage visualization in a virtual setup. 6 virtual servers connected to the dashboard.
        </p>
        <div class="collapse" id="collapse-gpu">
          <p class="bd-text" style="margin-bottom: 0px;">
            Better visuals of <code style="color: rgb(74, 222, 66);">nvidia-smi</code> command.
            What <code style="color: white;">nvidia-smi</code> command does is it shows the GPU utilization and memory usage in numbers in a terminal window.
            It is not an intuitive view to understand the complete state of a GPU server.
            What does this visualization tool then provide:
          </p>
          <ul class="bd-text" style="text-align: left;">
            <li>Fast understanding of the state of multiple GPU servers
              <ul>
                <li>View of unlimited number of servers on a single page</li>
              </ul>
            </li>
            <li>Real-time monitoring web dashboard</li>
            <li>User specific statistics</li>
            <li>User sorted GPU programs and their execution times</li>
          </ul>
          <p class="bd-text">
            With the software, it is easier for the machine learning team to distribute workload evenly and stop training experiments which are not needed anymore.
          </p>
          <p class="bd-text">
            Used technologies: Python, Flask, HTML, CSS, JS, Highcharts, Shell, and threading.
          </p>
          <p class="ref-text">
            2020
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-gpu" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-gpu" aria-expanded="false" aria-controls="collapse-gpu">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6 text-center">Remote device control</h2>
        <div class="img-border aspect-ratio-keeper">
          <iframe src="https://www.youtube.com/embed/drcwjqq63-M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="img-text">
          Real-time remote LED control with the mobile phone app, Raspberry Pi and a relay card.
        </p>
        <div class="collapse" id="collapse-remote">
          <p class="bd-text">
            Turning electric devices (up to 2kW) on/off remotely, also known as home automation.
            Being able to control your electric devices remotely might help you save time of not transporting to your home, cottage or office.
          </p>
          <p class="bd-text">
            I created a platform where users can login with Google account authentication and access multiple locations which to control remotely.
          </p>
          <p class="bd-text" style="margin-bottom: 0px;">
            Overview of the tech:
          </p>
          <ul class="bd-text" style="text-align: left;">
            <li>Relay card control through the GPIO pins of Raspberry Pi</li>
            <li>MongoDB for user registration and session management</li>
            <li>Routing between client, proxy server, backend server, and a terminal device</li>
            <li>The terminal device controls a relay card that turns the devices either on or off</li>
            <li>Used Node.js, Flask, HTML, JS, CSS, Python, and MongoDB</li>
          </ul>
          <p class="ref-text">
            2018
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-remote" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-remote" aria-expanded="false" aria-controls="collapse-remote">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6 text-center">Smart mirror</h2>
        <video class="img-border" width="100%" autoplay loop muted src="https://github-videos.s3.eu-west-2.amazonaws.com/smart_mirror_small.mp4" onclick="openModal(this.src, 'video')"></video>
        <p class="img-text">
          Embedded text of the weather, time, and news in a mirror.
        </p>
        <div class="collapse" id="collapse-smart-mirror">
          <p class="bd-text">
            Mornings are hectic ‒ you're rushing to get ready but still want to check the weather, local news, and the time without juggling multiple devices.
            A Smart Mirror solves this by displaying all that information right on your mirror while you get prepared for the day.
          </p>
          <p class="bd-text" style="margin-bottom: 0px;">
            Overview of the HW:
          </p>
          <ul class="bd-text" style="text-align: left;">
            <li>Raspberry Pi</li>
            <li>Monitor</li>
            <li>Reflective window film</li>
          </ul>
          <p class="bd-text" style="margin-bottom: 0px;">
            Overview of the SW:
          </p>
          <ul class="bd-text" style="text-align: left;">
            <li>Weather from pirate-weather API</li>
            <li>News from Kauppalehti</li>
          </ul>
          <img class="img-border" width="100%" src="https://s3.eu-west-2.amazonaws.com/github-videos/smart_mirror_front.JPG" onclick="openModal(this.src, 'image')">
          <p class="img-text">
            Front view of the smart mirror.
          </p>
          <img class="img-border" width="100%" src="https://s3.eu-west-2.amazonaws.com/github-videos/smart_mirror_back.JPG" onclick="openModal(this.src, 'image')">
          <p class="img-text">
            Back view of the smart mirror.
          </p>
          <p class="ref-text">
            2019
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-smart-mirror" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-smart-mirror" aria-expanded="false" aria-controls="collapse-smart-mirror">Read more &darr;</button></p>
      </div>
    </div>
  </div>

  <!-- Modal for images and videos -->
  <div id="myModal" class="modal">
    <span class="close" onclick="closeModal()">&times;</span>
    <img class="modal-content" id="img01">
    <video class="modal-content" id="video01" controls></video>
  </div>

</main>

<script src="assets/dist/js/bootstrap.bundle.min.js"></script>
<script>
  function readButtonChange(button_id) {
    var button_text = document.getElementById(button_id);
    var content = document.getElementById("collapse-".concat('', button_id.slice(-1)));
    
    if (button_text.innerHTML.lastIndexOf("less") >= 0) {
      button_text.innerHTML = "Read more &darr;";
    } else {
      button_text.innerHTML = "Read less &uarr;";
    }
  }

  function isMobile() {
    return /Mobi|Android|iPhone|iPad|iPod/i.test(navigator.userAgent);
  }

  function openModal(src, type) {
    if (isMobile()) {
      return;
    }

    var modal = document.getElementById("myModal");
    var img = document.getElementById("img01");
    var video = document.getElementById("video01");
    
    // Set up the modal content
    if (type === 'image') {
      img.src = src;
      img.style.display = "block";
      video.style.display = "none";
      // Remove show class initially
      img.classList.remove("show");
    } else if (type === 'video') {
      video.src = src;
      video.style.display = "block";
      img.style.display = "none";
      // Remove show class initially
      video.classList.remove("show");
    }
    
    // Show modal and set up flexbox centering
    modal.style.display = "flex";
    modal.style.alignItems = "center";
    modal.style.justifyContent = "center";
    
    // Remove show class from modal initially
    modal.classList.remove("show");
    
    // Force a reflow to ensure the modal is rendered without the show class
    modal.offsetHeight;
    
    // Add show class to trigger animations
    modal.classList.add("show");
    
    // Add show class to content after a small delay for staggered animation
    setTimeout(function() {
      if (type === 'image') {
        img.classList.add("show");
      } else if (type === 'video') {
        video.classList.add("show");
      }
    }, 100);
    
    // Add event listener to close modal when clicking outside
    window.onclick = function(event) {
      if (event.target == modal) {
        closeModal();
      }
    }
    
    // Add event listener for ESC key to close modal
    document.addEventListener('keydown', function(event) {
      if (event.key === "Escape") {
        closeModal();
      }
    });
  }

  function closeModal() {
    var modal = document.getElementById("myModal");
    var img = document.getElementById("img01");
    var video = document.getElementById("video01");
    
    // Remove show classes to trigger closing animation
    modal.classList.remove("show");
    img.classList.remove("show");
    video.classList.remove("show");
    
    // Wait for animation to complete before hiding the modal
    setTimeout(function() {
      modal.style.display = "none";
      img.src = "";
      video.src = "";
    }, 300); // Match the CSS transition duration
    
    // Remove the window click event listener
    window.onclick = null;
  }
</script>
<script src="assets/dist/js/animation.js"></script>
  </body>
</html>
