<!doctype html>
<html lang="en" data-bs-theme="auto" style="height: 100%;">
  <head>
    <script src="assets/js/color-modes.js"></script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Backpropagation, deepfake detection, segmentation, motion prediction, autonomous vehicles, molecular translation">
    <title>Projects</title>
    <link rel="icon" href="assets/pt_icon_dark_small.png" type="image/x-icon">
    <link href="assets/dist/css/bootstrap.min.css" rel="stylesheet">

    <style>
      .img-border {
        border-radius: 20px;
        margin-bottom: 15px;
      }

      .aspect-ratio-keeper {
        position: relative;
        width: 100%;
        padding-bottom: 56.25%; /* 16:9 aspect ratio */
        overflow: hidden;
      }

      .aspect-ratio-keeper iframe {
        position: absolute;
        width: 100%;
        height: 100%;
        border: 0;
      }

      a {
        text-decoration: none;
      }

      div.background {
        background: linear-gradient(rgba(255,255,255,0.4), rgba(255,255,255,0.4)), url('assets/alps_small.jpg');
        background-size: cover;
      }

      .bd-text {
        font-size: 18px;
        line-height: 27px;
        font-weight: 350;
        margin-bottom: 24px;
        letter-spacing: 0px;
        font-family: sans-serif;
        text-align: left;
      }

      .img-text {
        font-size: 15px;
        line-height: 21px;
        font-weight: 250;
        margin-bottom: 24px;
        letter-spacing: 0px;
        font-family: sans-serif;
        text-align: left;
      }

      .ref-text {
        font-size: small;
        line-height: 21px;
        font-weight: 350;
        margin-bottom: 24px;
        letter-spacing: 0px;
        font-family: sans-serif;
        text-align: left;
      }

      @media (max-width: 768px) {
        .bd-text {
          font-size: 16px;
          line-height: 24px;
          margin-bottom: 16px;
        }

        .img-text {
          font-size: 13px;
          line-height: 18px;
          margin-bottom: 11px;
        }
      }
    </style>
    
    <!-- Custom styles for this template -->
    <link href="assets/dist/css/custom.css" rel="stylesheet">
  </head>
  <body style="height: 100%;">

<!-- NAVBAR -->
<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
  <div class="container">
    <span class="nav-item"><a class="nav-link" href="/"><img width="28" height="28" src="assets/pt_icon_light.png" onmouseover="this.src='assets/pt_icon_lighter.png';" onmouseout="this.src='assets/pt_icon_light.png';"></a></span>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarsExample08" aria-controls="navbarsExample08" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    
    <div class="collapse navbar-collapse justify-content-md-center" id="navbarsExample08">
      <ul class="navbar-nav">
        <li class="nav-item" style="padding-right: 125px;"><a class="nav-link" href="/">Intro</a></li>
        <li class="nav-item" style="padding-right: 125px;"><a class="nav-link" href="experience">Experience</a></li>
        <li class="nav-item" style="padding-right: 125px;"><a class="nav-link" href="projects">Projects</a></li>
        <li class="nav-item"><a class="nav-link" href="education">Education</a></li>
      </ul>
    </div>
  </div>
</nav>

<main style="max-width: 1800px; margin-left: auto; margin-right: auto;">

  <!-- Top headline -->
  <div class="background position-relative overflow-hidden p-3 p-md-5 m-md-3 text-center bg-body-tertiary">
    <div class="col-md-6 p-lg-5 mx-auto my-5">
      <h1 class="display-3 fw-bold">Projects</h1>
    </div>
  </div>

  <!-- The first 3 projects -->
  <div class="d-md-flex flex-md-equal w-100 my-md-3 ps-md-3">
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Backpropagation</h2>
        <img class="img-border" width="100%" src="https://user-images.githubusercontent.com/33998401/274561674-b0de8b2e-3e7a-4538-befc-aec0873632de.gif">
        <p class="img-text">
          Learning process of a single neuron. Included components: 2 weights, bias, ReLU, mean squared error, and gradient descent.
        </p>
        <div class="collapse" id="collapse-backpropagation">
          <h4>Introduction</h4>
          <p class="bd-text">
            A machine learning model includes model trainable parameters ‒ weights and biases ‒ activation functions, and skip connections. The feed forward process of applying a ML model to inputs is called <em>Forward propagation</em>. Training a ML model requires a learning phase to update the weights and biases. This learning phase is called <em><a href="https://en.wikipedia.org/wiki/Backpropagation">Backpropagation</a></em>.
          </p>
          <h4>Implementation</h4>
          <p class="bd-text" style="margin-bottom: 0px;">
            In this project, I got into <em><a href="https://en.wikipedia.org/wiki/First_principle">First principles</a></em> of deep learning. I coded an own implementation of a neural network framework without using any common deep learning frameworks. The features which I implemented with Python were:
          </p>
          <ul class="bd-text" style="text-align: left;">
            <li>General</li>
            <ul class="bd-text" style="text-align: left; margin-bottom: 0px;">
              <li>Forward propagation</li>
              <li>Backpropagation</li>
              <li>Network visualization</li>
            </ul>
            <li>Layers</li>
            <ul class="bd-text" style="text-align: left; margin-bottom: 0px;">
              <li>Fully connected</li>
            </ul>
            <li>Activation functions</li>
            <ul class="bd-text" style="text-align: left; margin-bottom: 0px;">
              <li>ReLU</li>
              <li>Softmax</li>
            </ul>
            <li>Loss functions</li>
            <ul class="bd-text" style="text-align: left; margin-bottom: 0px;">
              <li>Mean squared error</li>
              <li>Cross entropy loss</li>
            </ul>
            <li>Optimizers</li>
            <ul class="bd-text" style="text-align: left; margin-bottom: 0px;">
              <li>Gradient descent</li>
            </ul>
          </ul>
          <h4>Theory</h4>
          <p class="bd-text">
            The main principle behind backpropagation is <em><a href="https://en.wikipedia.org/wiki/Derivative">Derivative</a></em>. To define how to update the neural network during an iteration, one needs to compute a gradient for each parameter. The gradient determines how much a parameter affects the loss, e.g. a parameter with gradient 0 contributes nothing to the loss. The gradient of a parameter \(p'\) is defined by the derivative of the loss \(l\) with respect to the parameter \(p\)
          </p>
          <p class="bd-text" style="text-align: center;">
            \[p' = \frac{dl}{dp}\]
          </p>
          <p class="bd-text">
            To compute the gradient of a parameter which has operations between the loss and the parameter, require <em><a href="https://en.wikipedia.org/wiki/Chain_rule">Chain rule</a></em>. The chain rule expresses the derivative of the composition of two differentiable functions in terms of the derivatives of the two separate functions. By knowing the derivative of a loss \(l\) with respect to a weight \(w_{1}\) and the derivate of the weight \(w_{1}\) with respect to a weight \(w_{0}\), then the derivative of \(l\) with respect to \(w_{0}\) will be:
          </p>
          <p class="bd-text" style="text-align: center;">
            \[\frac{dl}{dw_{0}} = \frac{dw_{1}}{dw_{0}} \cdot \frac{dl}{dw_{1}}\]
          </p>
          <p class="bd-text">
            To find the derivative of a weight multiplication and a bias addition functions, we could use the definition of a differentiable function and calculate the <a href="https://en.wikipedia.org/wiki/Limit_(mathematics)">limit</a> \(L\):
          </p>
          <p class="bd-text" style="text-align: center;">
            \[L = \lim\limits_{h \to 0} \frac{f(a + h) - f(a)}{h}\]
          </p>
          <p class="bd-text">
            In a weight multiplication, the input \(i\) is multiplied with a weight \(w\), hence the function will be \(f(w) = iw\). Now, we can use the definition of a differentiable function and calculate the derivative of \(f(w)\):
          </p>
          <p class="bd-text" style="text-align: center;">
            $$
            \begin{align}
              f'(w) &= \lim\limits_{h \to 0} \frac{i(w+h) - iw}{h}\\
              &= \lim\limits_{h \to 0} \frac{iw + ih - iw}{h}\\
              &= \lim\limits_{h \to 0} \frac{ih}{h}\\
              &= i
            \end{align}
            $$
          </p>
          <p class="bd-text">
            As seen, the "local derivative" of an input multiplied by a weight with respect to the weight is the value of the input. To obtain the final derivative which we need ‒ the derivative of the loss with respect to the weight ‒ the "local derivative" has to be multiplied with the rest of the chain rule derivatives.
          </p>
          <p class="bd-text">
            Similarly, let's derive the local derivative of a bias addition. In a neuron, a bias \(b\) is added to the outputs of weight multiplications \(o\). Thus, the derivate of the function \(f(b) = o + b\) will be: 
          </p>
          <p class="bd-text" style="text-align: center;">
            $$
            \begin{align}
              f'(b) &= \lim\limits_{h \to 0} \frac{o + (b+h) - (o+b)}{h}\\
              &= \lim\limits_{h \to 0} \frac{o + b + h - o - b}{h}\\
              &= \lim\limits_{h \to 0} \frac{h}{h}\\
              &= 1
            \end{align}
            $$
          </p>
          <p class="bd-text">
            Here, the local derivative is 1. Therefore, the derivative of the loss with respect to a bias is 1 multiplied by the rest of the chain rule derivatives.
          </p>
          <p class="bd-text">
            The derivatives of complex activation functions one could just search online and implement to the code. That's what I did.
          </p>
          <p class="bd-text">
            Finally, when the gradients are computed for each parameter, it's time to apply them and update the parameters. The simplest optimizer used to apply the gradients is <em><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient descent</a></em>. The derivative shows the direction of growth but when training a neural network, we want to minimize the loss hence take a step (<em><a href="https://en.wikipedia.org/wiki/Learning_rate">Learning rate</a></em>) to the direction of a negative gradient. That means, using gradient descent the updated value of a parameter \(p\) is:
            \[p_{u} = - g \cdot r \cdot p_{o}\]
            where \(p_{u}\) is the updated value of a parameter, \(g\) is the gradient, \(r\) is the learning rate, and \(p_{o}\) is the old value of the parameter.
          </p>
          <h4>Interesting findings</h4>
          <p class="bd-text">
            While debugging and visualizing the reason why my network didn't learn I came across <em><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Potential_problems">Dying ReLU</a></em> problem. Some units may always output zero (they are "dead" neurons) and don't update their weights during training. To address this problem, variations of ReLU have been proposed, such as Leaky ReLU, Parametric ReLU (PReLU), and Exponential Linear Unit (ELU), which aim to allow a small gradient for negative inputs to prevent neurons from becoming completely inactive.
          </p>
          <p class="ref-text">
            2023 | <a href="https://github.com/pettod/backpropagation">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-brackpropagation" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-backpropagation" aria-expanded="false" aria-controls="collapse-backpropagation">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Deepfake detection</h2>
        <img class="img-border" width="100%" src="https://github.com/pettod/deepfake-recognition-kaggle/assets/33998401/f15c348e-7b6c-40af-8f16-48699016f3cd">
        <p class="img-text">
          Real-time deepfake face detection and classification. See the moving ghost between the faces causing fake classification.*
        </p>
        <div class="collapse" id="collapse-deepfake">
          <p class="bd-text">
            Deepfake techniques, which present realistic AI-generated videos of people doing and saying fictional things, have the potential to have a significant impact on how people determine the legitimacy of information presented online.
            These content generation and modification technologies may affect the quality of public discourse and the safeguarding of human rights — especially given that deepfakes may be used maliciously as a source of misinformation, manipulation, harassment, and persuasion.
          </p>
          <p class="bd-text" style="margin-bottom: 0px;">
            I created a deep learning model to identify videos with facial or voice manipulations.
            The pipeline from input to output was as follows:
          </p>
          <ol class="bd-text" style="text-align: left;">
            <li>Decode .mp4 video into frames</li>
            <li>Detect faces from multiple frames</li>
            <li>Cluster different humans in a video</li>
            <li>Remove outliers that are not real humans</li>
            <li>Classify the video by using multiple faces from different video frames</li>
          </ol>
          <p class="bd-text">
            The challenge lied in how multi-step the classification had to be.
            Before being able to classify the video, had to create a face detection algorithm.
            There was a time limit so the face detection had to be fast but also accurate and not every model was able to meet these requirements.
            Also, in some of the videos there were multiple people present.
            Obviously, you cannot mess the different faces because that would easily lead to a fake prediction.
            So clustering was needed.
            Then sometimes the face detection picked face looking areas such as a pillow with eyes and a mouth or a face picture on a wall.
            These also had to be filtered out to avoid misleading the classification model.
            Finally, it was time for the classification model research in which, in the end, I concatenated 9 faces to an input image and fed it to the model.
          </p>
          <p class="ref-text">
            *Video detection from the public dataset: <a href="https://arxiv.org/abs/2006.07397">https://arxiv.org/abs/2006.07397</a>
            <br>
            2020 | <a href="https://github.com/pettod/deepfake-recognition-kaggle">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-deepfake" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-deepfake" aria-expanded="false" aria-controls="collapse-deepfake">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Package delivery optimization</h2>
        <img class="img-border" width="100%" src="https://github.com/pettod/delivery-routing/assets/33998401/8bda2271-aa07-48fe-8a4d-fd0f64ebcc97">
        <p class="img-text">
          Finding optimal route with 301 locations and 4 delivery vehicles.
        </p>
        <div class="collapse" id="collapse-package">
          <p class="bd-text" style="margin-bottom: 0px;">
            Challenged myself on learning how complex vehicle routing really is.
            The underlying problem is to minimize the traveling distance by taking into account constraints and changing environment.
            The simplest form of minimizing the distance without any constraints is called <a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem">Traveling Salesperson Problem</a> (TSP).
            Here are some numbers to understand the complexity of TSP:
          </p>
          <ul class="bd-text" style="text-align: left;" style="margin-bottom: 0px;">
            <li>With 4 locations there are (4 &ndash; 1)! = 6 possible routes</li>
            <li>With 11 locations there are (11 &ndash; 1)! = 3,628,800 possible routes</li>
            <li>With 21 locations there are (21 &ndash; 1)! = 2,432,902,008,176,640,000 possible routes</li>
          </ul>
          <p class="bd-text" style="margin-bottom: 0px;">
            Adding constraints to the routing problem further complicates the optimization.
            Constraints of the problem include, for example, vehicle capacity, vehicle refill points, traffic jams, cancelled deliveries, or product type limitations.
            I coded a solution taking into account the following constraints:
          </p>
          <ul class="bd-text" style="text-align: left;">
            <li>Minimize traveling distance</li>
            <li>N number of vehicles</li>
            <li>M number of packages</li>
            <li>Delivery deadlines of the packages</li>
            <li>Driver's working hours</li>
            <li>Driver's maximum single delivery distance</li>
            <li>Driver's mandatory lunch break</li>
          </ul>
          <p class="ref-text">
            2023 | <a href="https://github.com/pettod/delivery-routing">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-package" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-package" aria-expanded="false" aria-controls="collapse-package">Read more &darr;</button></p>
      </div>
    </div>
  </div>

  <!-- The second 3 projects -->
  <div class="d-md-flex flex-md-equal w-100 my-md-3 ps-md-3">
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Prostate cancer grade assessment</h2>
        <img class="img-border" width="100%" src="https://github.com/pettod/personal-website/assets/33998401/ac116f86-61a6-4a3a-934e-5fd22da48c1a">
        <p class="img-text">
          Input of 4x4 concatenated blocks of tissue area with respective predicted and ground truth Gleason segmentation.*
        </p>
        <div class="collapse" id="collapse-cancer">
          <p class="bd-text">
            With more than 1 million new diagnoses reported every year, prostate cancer (PCa) is the second most common cancer among males worldwide that results in more than 350,000 deaths annually.
            The key to decreasing mortality is developing more precise diagnostics.
          </p>
          <p class="bd-text">
            I created a deep learning model for diagnosing PCa from high-resolution images (average of 800 Mpx per image, maximum size >4000 Mpx).
          </p>
          <p class="bd-text">
            Achieved validation kappa score 0.91 (6 ISUP grades in the Gleason grading system).
            Segmented cell with normal, healthy, stroma and Gleason scores from 3 to 5.
            Classified by using as large tissue area as possible.
          </p>
          <p class="ref-text">
            *Image segmentation from the public dataset: <a href="https://www.nature.com/articles/s41591-021-01620-2">https://www.nature.com/articles/s41591-021-01620-2</a>
            <br>
            2020 | <a href="https://github.com/pettod/medical-segmentation">Github 1</a>, <a href="https://github.com/pettod/prostate-cancer-grade-assessment">Github 2</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-cancer" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-cancer" aria-expanded="false" aria-controls="collapse-cancer">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Motion prediction for autonomous vehicles</h2>
        <img class="img-border" width="100%" src="https://github.com/pettod/personal-website/assets/33998401/d2cbc66a-544d-48ff-9a47-cd885208982e">
        <p class="img-text">
          2D view of roads, traffic agents, and their trajectories.*
        </p>
        <div class="collapse" id="collapse-lyft">
          <p class="bd-text">
            Deep learning model for an autonomous vehicle to predict the movement of traffic agents such as cars, cyclists, and pedestrians.
            The target was to predict future trajectory from a given vector of (x,y) coordinates of the past trajectory.
          </p>
          <p class="bd-text">
            By converting the coordinates to a bird eye view image, the approach of dealing with vector data was changed to image to image problem resulting in higher accuracy.
          </p>
          <p class="ref-text">
            *Image simulated from the public dataset: <a href="https://arxiv.org/abs/2006.14480">https://arxiv.org/abs/2006.14480</a>
            <br>
            2020 | <a href="https://github.com/pettod/lyft-kaggle">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-lyft" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-lyft" aria-expanded="false" aria-controls="collapse-lyft">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Molecular translation</h2>
        <img class="img-border" width="100%" src="https://github.com/pettod/bms-molecular-translation/assets/33998401/7d1d3d97-3660-4169-ae1d-75bbb0c1a828">
        <p class="img-text">
          Drawn Skeletal formula of an input molecule and a corresponding ground truth InChI text.*
        </p>
        <div class="collapse" id="collapse-inchi">
          <p class="bd-text">
            Sometimes the best and easiest tools are still pen and paper.
            Organic chemists frequently draw out molecular work with the <a href="https://en.wikipedia.org/wiki/Skeletal_formula">Skeletal formula</a>, a structural notation used for centuries.
            Recent publications are also annotated with machine-readable chemical descriptions (<a href="https://en.wikipedia.org/wiki/International_Chemical_Identifier">InChI</a>), but there are decades of scanned documents that can't be automatically searched for specific chemical depictions.
            To speed up research and development efforts, an automated recognition of optical chemical structures will be helpful.
          </p>
          <p class="bd-text">
            I created a deep learning model to translate chemical images to InChI text.
          </p>
          <p class="ref-text">
            *Image sample from the public dataset: <a href="https://www.kaggle.com/competitions/bms-molecular-translation/data">https://www.kaggle.com/competitions/bms-molecular-translation/data</a>
            <br>
            2021 | <a href="https://github.com/pettod/bms-molecular-translation">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-inchi" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-inchi" aria-expanded="false" aria-controls="collapse-inchi">Read more &darr;</button></p>
      </div>
    </div>
  </div>

  <!-- The third 3 projects -->
  <div class="d-md-flex flex-md-equal w-100 my-md-3 ps-md-3">
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 text-center overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6">Charuco corner detection</h2>
        <img class="img-border" width="100%" src="https://github.com/pettod/charuco-corner-detection/raw/master/document_images/detected_charuco_markers.gif">
        <p class="img-text">
          Real-time Charuco corner detection. All of the captured pictures (even without seeing or detecting every corner) can be used for camera calibration because every corner has an ID.
        </p>
        <div class="collapse" id="collapse-charuco">
          <p class="bd-text">
            Tool for camera calibration to detect checkerboard corners with corner identifications.
            It is especially helpful for multi-camera calibration where all the cameras cannot see the whole checkerboard.
            By knowing the corner identifications, one can use images with partly visible checkerboards for camera calibration.
          </p>
          <p class="bd-text">
            Used Python OpenCV and embedded it into Matlab as well.
          </p>
          <p class="ref-text">
            2019 | <a href="https://github.com/pettod/charuco-corner-detection">Github</a>
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-charuco" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-charuco" aria-expanded="false" aria-controls="collapse-charuco">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6 text-center">GPU usage visualization</h2>
        <div class="img-border aspect-ratio-keeper">
          <iframe src="https://www.youtube.com/embed/qLBvez84VoA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="img-text">
          Real-time GPU usage visualization in a virtual setup. 6 virtual servers connected to the dashboard.
        </p>
        <div class="collapse" id="collapse-gpu">
          <p class="bd-text" style="margin-bottom: 0px;">
            Better visuals of <code style="color: rgb(74, 222, 66);">nvidia-smi</code> command.
            What <code style="color: white;">nvidia-smi</code> command does is it shows the GPU utilization and memory usage in numbers in a terminal window.
            It is not an intuitive view to understand the complete state of a GPU server.
            What does this visualization tool then provide:
          </p>
          <ul class="bd-text" style="text-align: left;">
            <li>Fast understanding of the state of multiple GPU servers
              <ul>
                <li>View of unlimited number of servers on a single page</li>
              </ul>
            </li>
            <li>Real-time monitoring web dashboard</li>
            <li>User specific statistics</li>
            <li>User sorted GPU programs and their execution times</li>
          </ul>
          <p class="bd-text">
            With the software, it is easier for the machine learning team to distribute workload evenly and stop training experiments which are not needed anymore.
          </p>
          <p class="bd-text">
            Used technologies: Python, Flask, HTML, CSS, JS, Highcharts, Shell, and threading.
          </p>
          <p class="ref-text">
            2020
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-gpu" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-gpu" aria-expanded="false" aria-controls="collapse-gpu">Read more &darr;</button></p>
      </div>
    </div>
    <div class="text-bg-dark me-md-3 pt-3 px-3 pt-md-5 px-md-5 overflow-hidden">
      <div class="my-3 py-3">
        <h2 class="display-6 text-center">Remote device control - web platform</h2>
        <div class="img-border aspect-ratio-keeper">
          <iframe src="https://www.youtube.com/embed/drcwjqq63-M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="img-text">
          Real-time remote LED control with the mobile phone app, Raspberry Pi and a relay card.
        </p>
        <div class="collapse" id="collapse-remote">
          <p class="bd-text">
            Turning electric devices (up to 2kW) on/off remotely, also known as home automation.
            Being able to control your electric devices remotely might help you save time of not transporting to your home, cottage or office.
          </p>
          <p class="bd-text">
            I created a platform where users can login with Google account authentication and access multiple locations which to control remotely.
          </p>
          <p class="bd-text" style="margin-bottom: 0px;">
            Overview of the tech:
          </p>
          <ul class="bd-text" style="text-align: left;">
            <li>Relay card control through the GPIO pins of Raspberry Pi</li>
            <li>MongoDB for user registration and session management</li>
            <li>Routing between client, proxy server, backend server, and a terminal device</li>
            <li>The terminal device controls a relay card that turns the devices either on or off</li>
            <li>Used Node.js, Flask, HTML, JS, CSS, Python, and MongoDB</li>
          </ul>
          <p class="ref-text">
            2018
          </p>
        </div>
        <p class="text-center gap-1"><button onclick="readButtonChange(this.id)" id="readMore-remote" class="btn btn-outline-light" type="button" data-bs-toggle="collapse" data-bs-target="#collapse-remote" aria-expanded="false" aria-controls="collapse-remote">Read more &darr;</button></p>
      </div>
    </div>
  </div>
  
</main>

<script src="assets/dist/js/bootstrap.bundle.min.js"></script>
<script>
  function readButtonChange(button_id) {
    var button_text = document.getElementById(button_id);
    var content = document.getElementById("collapse-".concat('', button_id.slice(-1)));
    
    if (button_text.innerHTML.lastIndexOf("less") >= 0) {
      button_text.innerHTML = "Read more &darr;";
    } else {
      button_text.innerHTML = "Read less &uarr;";
    }
  } 
</script>
  </body>
</html>
